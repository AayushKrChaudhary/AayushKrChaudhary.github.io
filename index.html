<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Aayush Kumar Chaudhary</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Aayush Kumar Chaudhary</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#timeline">Timeline</a>
              </li>
                        <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#research">Research Experience</a>
              </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>

        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">
<!-- <a href="./pdf/aayush.pdf" target="_blank">CV</a> -->
      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Aayush
            <span class="text-primary">Chaudhary</span>
          </h1>
          <div class="subheading mb-5">Bellevue, WA 98007 · (585) 430-0275 ·
            <a href="mailto:name@email.com">aayushkumarchaudhary@gmail.com</a>
          </div>
          <p class="mb-5"> I currently work on improving the Display Pipeline architectures and imaging algorithms for the next generation devices at Apple. I have received a Ph.D. degree from the Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology (RIT). My Ph.D. with Prof. Jeff Pelz ranges from low level image processing and machine learning to advanced computer vision and deep learning approaches to improve accuracy and precision of eye-tracking systems for real human subjects. Prior to working at Apple, I was working with Microsoft (Mixed reality) team to improve the eye tracking and iris recognition system for Hololens.


          </p>

          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
<a href="./pdf/CV.pdf">CV</a> 
            </li>

            <li class="list-inline-item">
              <a href="#">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/aayush-kumar-chaudhary-a5172a85">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/AayushKrChaudhary">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
                          </li>
            <li class="list-inline-item">
              <a href="https://bitbucket.org/%7Be49667bb-b54f-4916-b5b5-29a19caf1ba6%7D/">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-bitbucket fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>


      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="timeline">
        <div class="my-auto">
          <h2 class="mb-5">Timeline</h2>
            <li>
              <i class="fa-li fa fa-check"></i>
             2023 (June 2023) : Joined Apple to work on imaging pipeline architectures and algorithms for next generation displays
              </li> 
            <li>
              <i class="fa-li fa fa-check"></i>
             2022 (April 2022) : Started working in Microsoft (Eye Sensing team - Mixed Reality) as a Scientist II. The roles include improvement of eye-tracking and iris recognition in Hololens.
              </li> 
            <li>
              <i class="fa-li fa fa-check"></i>
             2022 (April 2022) : I successfully defended by Ph.D. dissertation.
              </li> 
            <li>
              <i class="fa-li fa fa-check"></i>
             2021 (June-September) : I completed my three month internship in QCT camera team at Qualcomm Technologies, Inc. My research was focused on " Designing neural network algorithms to solve research problems related to image restoration, and enhancements for improving photographs by constructing efficient solutions using integer arithmetic processors."
              </li> 
            <li>
              <i class="fa-li fa fa-check"></i>
             2021 (April) : Our paper titled 'EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking' was among the 11 best papers for IEEE-VR 2021 (Journal Track: Published in TVCG)
              </li> 
            <li>
              <i class="fa-li fa fa-check"></i>
             2021 (March) : Our two papers titled 'Semi Supervised Learning for Eye Image Segmentation' and 'Enhancing the precision of remote eye-tracking using iris velocity estimation' are accepted in ACM Symposium on Eye Tracking Research & Applications (ETRA 2021)
              </li> 

            <li>
              <i class="fa-li fa fa-check"></i>
             2021 (January) : Our paper titled 'EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking' is accepted in IEEE Transactions on Visualization and Computer Graphics special issue for IEEE-VR.
              </li> 

            <li>
              <i class="fa-li fa fa-check"></i>
             2020 (September) : Our paper titled 'RIT-Eyes: Rendering of near-eye images for eye-tracking applications' got the best paper honorable mention in SAP '20 Proceedings of the ACM Symposium on Applied Perception 
              </li> 

            <li>
              <i class="fa-li fa fa-check"></i>
             2020 (June) : Our paper titled 'RIT-Eyes: Rendering of near-eye images for eye-tracking applications' is accepted in SAP '20 Proceedings of the ACM Symposium on Applied Perception 
              <a target="_blank" style="color: #500;" href="https://cs.rit.edu/~cgaplab/RIT-Eyes/">RIT-Eyes Dataset and Demo Video</a></li> 

            <li>
              <i class="fa-li fa fa-check"></i>
             2020 (March) : Our paper titled 'Privacy-Preserving Eye Videos using Rubber Sheet Model' is accepted in ETRA '20 Proceedings of the 12th ACM Symposium on Eye Tracking Research & Applications </li>

            <li>
              <i class="fa-li fa fa-check"></i>
             2019 (November) : Received Prof.F.N. Trofimenkoff Academic Achievement Award from Department of Electronics and Computer Engineering, Pulchowk Campus, IOE for academic success during undergraduate studies. </li>

            <li>
              <i class="fa-li fa fa-check"></i>
             2019 (November) : Presented our work 'RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking' in ICCVW'2019 [Talk]</li>

            <li>
              <i class="fa-li fa fa-check"></i>
             2019 (September) : Our team RIT-MVRL wins Facebook's OpenEDS Semantic Segmentation Challenge. Our paper titled 'RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking' is accepted in ICCVW'2019</li>
            <li>
              <i class="fa-li fa fa-check"></i>
             2019 (June) : Attended the 11th ACM Symposium on Eye Tracking Research & Applications (ETRA). I presented my Ph.D. proposal. </li>
            <li>
              <i class="fa-li fa fa-check"></i>
             2019 (March) : Our Journal paper titled 'Motion tracking of iris features to detect small eye movements' is accepted in Journal of Eye Movement Research (JEMR)</li>
            <li>
              <i class="fa-li fa fa-check"></i>
             2019 (March) : 'Motion Tracking of Iris Features for Eye Tracking' is accepted in 2019 Symposium on Eye Tracking Research and Applications (ETRA '19)</li>            
          <li>
              <i class="fa-li fa fa-check"></i>
              2017 (June) : Started Working with Prof. Jeff Pelz in Multidisciplinary Vision Lab </li>
            <li>
              <i class="fa-li fa fa-check"></i>
              2017 (June) : Passed the comprehensive examination for Ph.D. </li>
                        <li>
              <i class="fa-li fa fa-check"></i>
              2016 (August) : Joined Rochester Institute of Technology for Ph.D. in Imaging Science</li>
                                  <li>
              <i class="fa-li fa fa-check"></i>
              2016 : Performance Award in recognition of High Performance for Q1’2016 , Samsung India
Electronics Pvt. Ltd., Nepal Office </li>
                        <li>
              <i class="fa-li fa fa-check"></i>
              2014 : Started working at Samsung India Electronics Pvt. Ltd (Nepal branch) as Senior Engineer </li>
        </div>
 </div>


      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="research">
        <div class="my-auto">
          <h2 class="mb-5">Research Experience</h2>

<ul>
    <li>
        Gaze Estimation
        <ul>
            <li>Study of feasibility of iris features based gaze estimation</li>
            <li>Developed a new approach of incorporating regression-based models with machine learning based approaches (based on proposed modified Kalman filter to disentangle useful pieces of information from two independent sources (one precise but drifting signal, and another accurate but noisy signal) </li>
            <li>Improved the precision and accuracy of eye trackers</li>
            <li>Designed and tested eye tracking hardware setup for study of various eye movements for human subjects

</li>
        </ul>
    </li>
</ul>


<ul>
    <li>
        Segmentation 
        <ul>
            <li>Improved the robustness of eye-parts segmentation with memory efficient models </li>
            <li>Designed novel end-to-end framework for ellipse segmentation on eye-parts </li>
            <li>Proposed novel frameworks for semi-supervised learning for eye segmentation </li>
            <li>Currently working on temporal and semi-supervised segmentation

</li>
     
        </ul>
    </li>
</ul>


<ul>
    <li>
        Privacy in eye videos
        <ul>
            <li>Designed concept for a novel approach to preserve privacy in the eye videos without degrading the accuracy of eye trackers </li>     
        </ul>
    </li>
</ul>



<ul>
    <li>
        Synthetic dataset for eye-tracking applications 
        <ul>
            <li>Collaborated with multiple research labs in RIT to introduce a synthetic eye image generation platform for eye-tracking applications  </li>
<iframe width="699" height="300" src="https://www.youtube.com/embed/SDVfsL6UGro" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
     
        </ul>
    </li>
</ul>


        </div>
 </div>

   </section>
            <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Publications</h2>


            <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                            <left><img src="images/ssl.png" width="70%" height="70%"></left>
                        </div>
                        <div class="seven columns main-col">
                                          <div style="background-color:FloralWhite;">
<strong>Semi-Supervised Learning for Eye Image Segmentation </strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em><strong>Aayush Chaudhary*</strong>, Prashnna Gyawali, Linwei Wang, Jeff Pelz</em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
           
Recent advances in appearance-based models have shown improved eye tracking performance in difficult scenarios like occlusion due to eyelashes, eyelids or camera placement, and environmental reflections on the cornea and glasses. The key reason for the improvement is the accurate and robust identification of eye parts (pupil, iris, and sclera regions). The improved accuracy often comes at the cost of labeling an enormous dataset, which is complex and time-consuming. This work presents two semi-supervised learning frameworks to identify eye-parts by taking advantage of unlabeled images where labeled datasets are scarce. With these frameworks, leveraging the domain-specific augmentation and novel spatially varying transformations for image segmentation, we show improved performance on various test cases. For instance, for a model trained on just 48 labeled images, these frameworks achieved an improvement of 0.38% and 0.65% in segmentation performance over the baseline model, which is trained only with the labeled dataset.
                            <div style="color:DodgerBlue;">ETRA '21 (short papers) Proceedings of ACM Symposium on Eye Tracking Research & Applications.</div>
                                <ul> 
                                    
                                    
                                      <li><a target="_blank" style="color: #500;" href="pdf/Chaudhary_SSL.pdf">Poster</a></li>
                                   
                                    <li><a target="_blank" style="color: #500;" href="https://arxiv.org/pdf/2103.09369.pdf">Paper</a></li>

                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />


              <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                            <left><img src="images/hybrid.png" width="40%" height="40%"></left>
                        </div>
                        <div class="seven columns main-col">
                                          <div style="background-color:FloralWhite;">
<strong>Enhancing the Precision of Eye Tracking using Iris Feature Motion Vectors </strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em><strong>Aayush Chaudhary</strong>,  Jeff Pelz</em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
           
A new high-precision eye-tracking method has been demonstrated recently by tracking the motion of iris features rather than by exploiting pupil edges. While the method provides high precision, it suffers from temporal drift, an inability to track across blinks, and loss of texture matches in the presence of motion blur. In this work, we present a new methodology to address these issues by optimally combining the information from both iris textures and pupil edges. With this method, we show an improvement in precision (S2S-RMS & STD) of at least 48% and 10% respectively while fixating a series of small targets and following a smoothly moving target. Further, we demonstrate the capability in the identification of microsaccades between targets separated by 0.2-degree.
                            <div style="color:DodgerBlue;">ETRA '21 (short papers) Proceedings of ACM Symposium on Eye Tracking Research & Applications.</div>
                                <ul> 
                                     <li><a target="_blank" style="color: #500;" href="pdf/Chaudhary_Pelz_hybrid.pdf">Poster</a></li>
                                   
                                    <li><a target="_blank" style="color: #500;" href="https://arxiv.org/abs/2009.09348">Paper</a></li>

                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />

            <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                            <left><img src="images/EllSeg.jpg" width="40%" height="40%"></left>
                        </div>
                        <div class="seven columns main-col">
                                          <div style="background-color:FloralWhite;">
<strong>EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking </strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em>Rakshit Kothari*, <strong>Aayush Chaudhary*</strong>, Reynold Bailey, Jeff Pelz, Gabriel Diaz</em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
           
Ellipse fitting, an essential component in pupil or iris tracking based video oculography, is performed on previously segmented eye parts generated using various computer vision techniques. Several factors, such as occlusions due to eyelid shape, camera position or eyelashes, frequently break ellipse fitting algorithms that rely on well-defined pupil or iris edge segments. In this work, we propose training a convolutional neural network to directly segment entire elliptical structures and demonstrate that such a framework is robust to occlusions and offers superior pupil and iris tracking performance (at least 10% and 24% increase in pupil and iris center detection rate respectively within a two-pixel error margin) compared to using standard eye parts segmentation for multiple publicly available synthetic segmentation datasets.
                            <div style="color:DodgerBlue;">IEEE Transactions on Visualization and Computer Graphics (TVCG) Special Issue on the 2021 IEEE Conference on Virtual Reality and 3D User Interfaces (VR).</div>
                                <ul> 
                                    
                                    <li><a target="_blank" style="color: #500;" href="https://arxiv.org/abs/2007.09600">Paper</a></li>
                                    <li><a target="_blank" style="color: #500;" href="https://www.youtube.com/watch?v=WmsSGSskaSw">Talk</a></li>
                                    <li><a target="_blank" style="color: #500;" href="https://cis.rit.edu/~rsk3900/EllSeg/">Code</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />
            <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                            <left><img src="pdf/Privacy.pdf" width="40%" height="40%"></left>
                        </div>
                        <div class="seven columns main-col">
                                          <div style="background-color:FloralWhite;">
<strong>Privacy-Preserving Eye Videos using Rubber Sheet Model </strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em><strong>Aayush Chaudhary</strong>, Jeff Pelz </em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
           
Video-based eye trackers estimate gaze based on eye images/videos. As security and privacy concerns loom over technological advancements, tackling such challenges is crucial. We present a new approach to handle privacy issues in eye videos by replacing the current identifiable iris texture with a different iris template in the video capture pipeline based on the Rubber Sheet Model. We extend to image blending and median-value representations to demonstrate that videos can be manipulated without significantly degrading segmentation and pupil detection accuracy.
                            <div style="color:DodgerBlue;">ETRA '20 Proceedings of the 12th ACM Symposium on Eye Tracking Research & Applications</div>
                                <ul> 


                                    <li><a target="_blank" style="color: #500;" href="pdf/Chaudhary_Pelz_Privacy.pdf">Poster</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />
            <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                            <left><img src="images/award.png" width="10%" height="10%"></left>
                        </div>
                        <div class="seven columns main-col">
                                                    <div style="background-color:FloralWhite;">
<strong>RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking</strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em><strong>Aayush Chaudhary*</strong>, Rakshit Kothari*, Manoj Acharya*, Sushil Dangi, NitinRaj Nair, Reynold Bailey, Chris Kanan, Gabriel Diaz, Jeff Pelz </em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
Accurate eye segmentation can improve eye-gaze estimation and support interactive computing based on visual attention; however, existing eye segmentation methods suffer from issues such as person-dependent accuracy, lack of robustness, and an inability to be run in real-time. Here, we present the RITnet model, which is a deep neural network that combines U-Net and DenseNet. RITnet is under 1 MB and achieves 95.3% accuracy on the 2019 OpenEDS Semantic Segmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at > 300Hz, enabling real-time gaze tracking applications. Pre-trained models and source code are available
                            <div style="color:DodgerBlue;">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</div>
                                <ul> 
                                    
                                    <li><a target="_blank" style="color: #500;" href="https://ieeexplore.ieee.org/document/9022181">Paper</a></li>
                                        <li><a target="_blank" style="color: #500;" href="https://research.fb.com/programs/openeds-challenge/">Winner of Semantic Segmentation Challenge Organized by Facebook</a></li> 
                                    <li><a target="_blank" style="color: #500;" href="https://www.rit.edu/news/rit-researchers-win-first-place-international-eye-tracking-challenge-facebook-research">RIT News</a></li> 
                                    <li><a target="_blank" style="color: #500;" href="images/RITnet_ICCV.pdf">Poster</a></li>
                                    <li><a target="_blank" style="color: #500;" href="https://bitbucket.org/eye-ush/ritnet/">Code</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />
            <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                            <left><img src="images/microsaccade.png" width="90%" height="90%"></left>
                        </div>
                        <div class="seven columns main-col">
                                          <div style="background-color:FloralWhite;">
<strong>Motion tracking of iris features to detect
small eye movements</strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em><strong>Aayush Chaudhary</strong>, Jeff Pelz </em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
           
The inability of current video-based eye trackers to reliably detect very small eye move-ments has led to confusion about the prevalence or even the existence of monocular mi-crosaccades (small, rapid eye movements that occur in only one eye at a time). As current
methods often rely on precisely localizing the pupil and/or corneal reflection on successive
frames, current microsaccade-detection algorithms often suffer from signal artifacts and a
low signal-to-noise ratio. We describe a new video-based eye tracking methodology which
can reliably detect small eye movements over 0.2 degrees (12 arcmins) with very high con-fidence. Our method tracks the motion of iris features to estimate velocity rather than posi-tion, yielding a better record of microsaccades. We provide a more robust, detailed record
of miniature eye movements by relying on more stable, higher-order features (such as local
features of iris texture) instead of lower-order features (such as pupil center and corneal
reflection), which are sensitive to noise and drift.
                            <div style="color:DodgerBlue;">Journal of Eye Movement and Research (JEMR 2019) </div>
                                <ul> 
                                    
                                    <li><a target="_blank" style="color: #500;" href="https://doi.org/10.16910/jemr.12.6.4">Paper</a></li>
        <!--                            <li><a target="_blank" style="color: #500;" href="https://github.com/sandeshgh/Improving-Generalization">Code</a></li> 
                                    <li><a target="_blank" style="color: #500;" href="presentations/IPMI_Presentation_Final.pdf">Talk</a></li>-->
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />
            <div class="row item">
                    <div class="twelve columns">
                        <div class="five columns header-col">
                         <!--     <left><img src="images/microsaccade.png" width="90%" height="90%"></left> -->  
                        </div>
                        <div class="seven columns main-col">
                            <div style="background-color:FloralWhite;">
<strong>Motion tracking of iris features </strong></div>
                            <div class="pubd"></div>
                            <div class="puba"><em><strong>Aayush Chaudhary</strong></em></div>
                            <!--<p style="color:green; font-size: 16px;"></p>-->
                            <div class="publ"> 
           
Current video-based eye trackers fail to acquire a high signal-to- noise (SNR) ratio which is crucial for specific applications like interactive systems, event detection, the study of various eye move- ments, and most importantly estimating the gaze position with high certainty. Specifically, current video-based eye trackers over-rely on precise localization of the pupil boundary and/or corneal reflec- tion (CR) for gaze tracking, which often results in inaccuracies and large sample-to-sample root mean square (RMS-S2S). Therefore, it is crucial to address the shortcomings of these trackers, and we plan to study a new video-based eye tracking methodology focused on simultaneously tracking the motion ofmany iris features and in- vestigate its implications for obtaining high accuracy and precision. In our preliminary work, the method has shown great potential for robust detection of microsaccades over 0.2 degrees with high confidence. Hence, we plan to explore and optimize this technique.
                            <div style="color:DodgerBlue;">2019 Symposium on Eye Tracking Research and Applications (ETRA ’19)</div>
                                <ul> 

                                    
                                    <li><a target="_blank" style="color: #500;" href="https://dl.acm.org/doi/10.1145/3314111.3322872">Paper</a></li>
                                    <li><a target="_blank" style="color: #500;" href="images/ETRA.pdf">Poster</a></li>
        <!--                            <li><a target="_blank" style="color: #500;" href="https://github.com/sandeshgh/Improving-Generalization">Code</a></li> 
                                    <li><a target="_blank" style="color: #500;" href="presentations/IPMI_Presentation_Final.pdf">Talk</a></li>-->
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
<hr style="height:2px;border:none;color:#0;background-color:#100;" />


        </div>
 </div>



          
      </section>
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
